{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Style Transfer Train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0HDAX91xcOA2","colab_type":"code","colab":{}},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, Dataset\n","\n","from tqdm.notebook import tqdm\n","from google.colab import drive\n","import numpy as np\n","import os\n","import sys\n","\n","manualSeed = 999\n","torch.manual_seed(manualSeed)\n","\n","drive.mount(\"/content/gdrive\")\n","path = \"/content/gdrive/My Drive/Colab Notebooks/GANS/Style Transfer\"\n","\n","\n","sys.path.append(path)\n","from Utils import networks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LBxpcfaNybWC","colab_type":"code","colab":{}},"source":["# Images Dataset that returns one style and one content image. As I only trained using 40.000\n","# images each, each image is randomly sampled. The way it is implemented does not allow multi-threading. However\n","# as this network is relatively small and training times low, no improved class was implemented.\n","\n","class Images(Dataset): \n","  def __init__(self, root_dir1, root_dir2, transform=None):\n","    self.root_dir1 = root_dir1\n","    self.root_dir2 = root_dir2\n","    self.transform = transform\n","\n","  def __len__(self):\n","    return min(len(os.listdir(self.root_dir1)), len(os.listdir(self.root_dir2)))\n","\n","  def __getitem__(self, idx):\n","    all_names1, all_names2 = os.listdir(self.root_dir1), os.listdir(self.root_dir2)\n","    idx1, idx2 = np.random.randint(0, len(all_names1)), np.random.randint(0, len(all_names2))\n","\n","    img_name1, img_name2 = os.path.join(self.root_dir1, all_names1[idx1]), os.path.join(self.root_dir2, all_names2[idx2])\n","    image1 = Image.open(img_name1).convert(\"RGB\")\n","    image2 = Image.open(img_name2).convert(\"RGB\")\n","\n","    if self.transform:\n","      image1 = self.transform(image1)\n","      image2 = self.transform(image2)\n","\n","    return image1, image2  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MIVw4MQo6WKu","colab_type":"code","colab":{}},"source":["# To note is that the images are not normalised\n","transform = transforms.Compose([transforms.Resize(512), \n","                               transforms.CenterCrop(256),\n","                               transforms.ToTensor()])\n","\n","\n","# Specify the path to the style and content images\n","pathStyleImages = \"/content/Data/Wiki_40k\"\n","pathContentImages = \"/content/Data/Coco_40k\" \n","\n","\n","all_img = Images(pathStyleImages, pathContentImages, transform=transform)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bGd2_znLxLIy","colab_type":"code","colab":{}},"source":["# Simple save \n","def save_state(decoder, optimiser, iters, run_dir):\n","  \n","  name = \"StyleTransfer Checkpoint Iter: {}.tar\".format(iters)\n","  torch.save({\"Decoder\" : decoder,\n","              \"Optimiser\" : optimiser,\n","              \"iters\": iters\n","              }, os.path.join(path, name))\n","  print(\"Saved : {} succesfully\".format(name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nNf9XPp2HgCO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600861736262,"user_tz":-120,"elapsed":825,"user":{"displayName":"Marvin Alberts","photoUrl":"","userId":"07281461137485771510"}}},"source":["def training_loop(network, # StyleTransferNetwork\n","                  dataloader_comb, # DataLoader\n","                  n_epochs, # Number of Epochs\n","                  run_dir # Directory in which the checkpoints and tensorboard files are saved\n","                  ):\n","  \n","\n","  writer = SummaryWriter(os.path.join(path, run_dir))\n","  # Fixed images to compare over time\n","  fixed_batch_style, fixed_batch_content = all_img[0]\n","  fixed_batch_style, fixed_batch_content =  fixed_batch_style.unsqueeze(0).to(device), fixed_batch_content.unsqueeze(0).to(device) # Move images to device\n","\n","  writer.add_image(\"Style\", torchvision.utils.make_grid(fixed_batch_style))\n","  writer.add_image(\"Content\", torchvision.utils.make_grid(fixed_batch_content))\n","\n","  iters = network.iters\n","\n","  for epoch in range(1, n_epochs+1):\n","    tqdm_object = tqdm(dataloader_comb, total=len(dataloader_comb))\n","\n","    for style_imgs, content_imgs in tqdm_object:\n","      network.adjust_learning_rate(network.optimiser, iters)\n","      style_imgs = style_imgs.to(device)\n","      content_imgs = content_imgs.to(device)\n","\n","      loss_comb, content_loss, style_loss = network(style_imgs, content_imgs)\n","\n","      network.optimiser.zero_grad()\n","      loss_comb.backward()\n","      network.optimiser.step()\n","\n","      # Update status bar, add Loss, add Images\n","      tqdm_object.set_postfix_str(\"Combined Loss: {:.3f}, Style Loss: {:.3f}, Content Loss: {:.3f}\".format(\n","                                  loss_comb.item()*100, style_loss.item()*100, content_loss.item()*100))\n","    \n","      if iters % 25 == 0:\n","        writer.add_scalar(\"Combined Loss\", loss_comb*1000, iters)\n","        writer.add_scalar(\"Style Loss\", style_loss*1000, iters)\n","        writer.add_scalar(\"Content Loss\", content_loss*1000, iters)\n","\n","      if (iters+1) % 2000 == 1:\n","        with torch.no_grad():\n","          network.set_train(False)\n","          images = network(fixed_batch_style, fixed_batch_content)\n","          img_grid = torchvision.utils.make_grid(images)\n","          writer.add_image(\"Progress Iter: {}\".format(iters), img_grid)\n","          network.set_train(True)\n","\n","      if (iters+1) % 4000 == 1:\n","          save_state(network.decoder.state_dict(), network.optimiser.state_dict(), iters, run_dir)\n","          writer.close()\n","          writer = SummaryWriter(os.path.join(path, run_dir))\n","\n","      iters += 1"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKg_C0K6leFp","colab_type":"code","colab":{}},"source":["device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","learning_rate = 1e-4\n","learning_rate_decay = 5e-5\n","\n","dataloader_comb = DataLoader(all_img, batch_size=5, shuffle=True, num_workers=0, drop_last=True)\n","gamma = torch.tensor([2]).to(device) # Style weight\n","\n","n_epochs = 5\n","run_dir = \"runs/Run 1\" # Change if you want to save the checkpoints/tensorboard files in a different directory\n","\n","state_encoder = torch.load(os.path.join(path, \"vgg_normalised.pth\"))\n","network = networks.StyleTransferNetwork(device,\n","                                        state_encoder,\n","                                        learning_rate,\n","                                        learning_rate_decay,\n","                                        gamma,\n","                                        load_fromstate=False,\n","                                        load_path=os.path.join(path, \"StyleTransfer Checkpoint Iter: 120000.tar\"))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_huZgs0ClUJ","colab_type":"code","colab":{}},"source":["training_loop(network, dataloader_comb, n_epochs, run_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4m-uIZxqXeoD","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}